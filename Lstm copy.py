import torch
import torch.nn as nn
import torch.optim as optim
import jieba

# ========== ä¸­æ–‡è¨“ç·´èªæ–™ ==========
text = """
è‡ºåŒ—ï¼Œä¸€å¦‚å¾€å¸¸åœ°ä¸‹è‘—æ¯›æ¯›ç´°é›¨ã€‚æ·é‹ç«™çš„äººæ½®ä¸å¤šä¹Ÿä¸å°‘ï¼Œæ—æ˜€é’ç«™åœ¨äººç¾¤ä¸­ï¼Œæ‰‹æ¡è‘—ä¸€æ¯å¿«æ¶¼æ‰çš„å’–å•¡ï¼Œçœ¼ç¥é£„å¿½ã€‚å¥¹æ­£è¦è¶•å¾€ä¸€å®¶å¥¹ä¸æ˜¯ç‰¹åˆ¥å–œæ­¡çš„å…¬å¸ï¼Œåšè‘—ä¸€ä»½å¥¹ä¸è¨å­ä½†ä¹Ÿèªªä¸ä¸Šç†±æ„›çš„å·¥ä½œã€‚

å°±åœ¨åˆ—è»ŠæŠµé”çš„æ™‚å€™ï¼Œä¸€å€‹ç©¿è‘—é»ƒè‰²é›¨è¡£çš„å°ç”·å­©çªç„¶è·Œå€’åœ¨æœˆå°é‚Šç·£ã€‚æ˜€é’æ¯«ä¸çŒ¶è±«è¡ä¸Šå‰ï¼Œä¸€æŠŠå°‡ä»–æ‹‰äº†å›ä¾†ï¼Œåˆ—è»Šæ“¦èº«è€Œéï¼Œé¢¨æ€èµ·å¥¹çš„é«®çµ²ã€‚

ã€Œä½ æ²’äº‹å§ï¼Ÿã€å¥¹å–˜è‘—æ°£å•ã€‚

å°ç”·å­©é»é»é ­ï¼Œçœ‹è‘—å¥¹èªªï¼šã€Œå¦³æ˜¯ä¸æ˜¯æœ¬ä¾†è¦æ­é€™ç­è»Šï¼Ÿã€

ã€Œæ˜¯å•Šï¼Œä½†æ²’é—œä¿‚ï¼Œé‚„æœ‰ä¸‹ä¸€ç­ã€‚ã€

å°ç”·å­©ç¬‘äº†ï¼Œçœ¼ç›äº®æ™¶æ™¶çš„ï¼šã€Œå¦³æ•‘äº†æˆ‘ï¼Œå¦³æœƒæœ‰å¥½é‹çš„ï¼ã€

æ˜€é’ä¸€æ„£ï¼Œæƒ³ç¬‘ï¼Œå»åœ¨ä¸‹ä¸€ç§’ç™¼ç¾é‚£å€‹å°ç”·å­©ä¸è¦‹äº†ã€‚å¥¹å››è™•å¼µæœ›ï¼Œäººç¾¤ä¾†ä¾†å»å»ï¼Œå½·å½¿ä»€éº¼éƒ½æ²’ç™¼ç”Ÿã€‚

é‚£å¤©ï¼Œå¥¹é²åˆ°äº†åŠå°æ™‚ã€‚ä¸»ç®¡è‡‰è‰²ä¸å¥½çœ‹ï¼Œæœƒè­°ä¹Ÿæ—©å·²çµæŸã€‚ä½†å¥‡å¦™çš„æ˜¯ï¼Œå¥¹ç«Ÿç„¶æ²’æœ‰è¢«è²¬ç½µï¼Œåè€Œè¢«å¦ä¸€å€‹éƒ¨é–€çš„ç¶“ç†æ³¨æ„åˆ°ï¼Œé‚€è«‹å¥¹åƒèˆ‡ä¸€å€‹æ–°çš„å°ˆæ¡ˆã€‚å°ˆæ¡ˆçš„å…§å®¹è®“å¥¹å¿ƒå‹•ï¼Œä¹…é•åœ°æ„Ÿå—åˆ°å·¥ä½œçœŸæ­£çš„æ„ç¾©ã€‚

å¹¾å¤©å¾Œï¼Œå¥¹èµ°åœ¨å›å®¶çš„è·¯ä¸Šï¼Œåˆè¦‹åˆ°é‚£å€‹é»ƒè‰²é›¨è¡£çš„ç”·å­©ï¼Œåœ¨ä¸€é–“æ›¸åº—å‰éœéœç«™è‘—ã€‚é€™æ¬¡ä»–è½‰èº«æœå¥¹ç¬‘äº†ç¬‘ï¼Œç„¶å¾ŒæŒ‡å‘æ›¸åº—è£¡çš„ä¸€æœ¬æ›¸ï¼šã€Šäººç”Ÿçš„ä¸‰æ¢è·¯ã€‹ã€‚

å¥¹èµ°é€²å»ï¼Œè²·ä¸‹é‚£æœ¬æ›¸ã€‚æ›¸çš„é–‹é ­å¯«è‘—ï¼šã€Œæœ‰äº›é¸æ“‡ï¼Œçœ‹ä¼¼å¾®å°ï¼Œå»æ±ºå®šäº†æˆ‘å€‘çš„ä¸€ç”Ÿã€‚ã€

æ¥ä¸‹ä¾†çš„æ—¥å­è£¡ï¼Œæ˜€é’é–‹å§‹ç•™æ„é‚£äº›ã€Œçœ‹ä¼¼å¾®å°ã€çš„é¸æ“‡ã€‚å¥¹é–‹å§‹èªªå‡ºè‡ªå·±çš„æ„è¦‹ã€ä¸å†å‹‰å¼·è‡ªå·±åŠ ç­ã€ä¸å†å’Œé‚£å€‹è®“å¥¹ç¸½æ˜¯æ„Ÿè¦ºç–²æ†Šçš„å‰ç”·å‹ç³¾çºã€‚æ¯ä¸€å€‹é¸æ“‡ï¼Œéƒ½åƒæ˜¯åœ¨èª¿æ•´å‘½é‹çš„æ–¹å‘ã€‚

ä¸‰å€‹æœˆå¾Œï¼Œé‚£å€‹å°ˆæ¡ˆæˆåŠŸäº†ï¼Œå¥¹ä¹Ÿè¢«å‡è·ã€‚å¥¹ç¬¬ä¸€æ¬¡æ„Ÿè¦ºè‡ªå·±åœ¨èµ°ä¸€æ¢çœŸæ­£å±¬æ–¼è‡ªå·±çš„è·¯ã€‚æŸå¤©å¤œæ™šï¼Œåœ¨å›å®¶çš„æ·é‹ä¸Šï¼Œå¥¹åˆè¦‹åˆ°äº†é‚£å€‹ç”·å­©ã€‚

ã€Œè¬è¬å¦³é‚£å¤©æ•‘äº†æˆ‘ã€‚ã€ä»–è¼•è²èªªã€‚

ã€Œä½ â€¦åˆ°åº•æ˜¯èª°ï¼Ÿã€å¥¹å°è²å•ã€‚

ç”·å­©çœ¨çœ¨çœ¼ï¼Œèªªï¼šã€Œæˆ‘æ˜¯å¦³å‘½é‹è£¡çš„ä¸€å€‹é¸æ“‡ã€‚é‚£å¤©å¦‚æœå¦³æ²’æœ‰åœä¸‹ä¾†ï¼Œå¦³æœƒæ­ä¸Šé‚£ç­è»Šï¼Œäººç”Ÿæœƒå¾€å¦ä¸€æ¢è·¯èµ°ã€‚ç¾åœ¨çš„å¦³ï¼Œæ¯”é‚£æ™‚å€™æ›´é è¿‘å¦³çœŸæ­£æƒ³æˆç‚ºçš„äººã€‚ã€

åˆ—è»ŠæŠµé”çµ‚é»æ™‚ï¼Œç”·å­©ä¸è¦‹äº†ã€‚åªç•™ä¸‹ä¸€å¼µä¾¿åˆ©è²¼è²¼åœ¨è»Šçª—ä¸Šï¼šã€Œç¹¼çºŒé¸æ“‡å¦³ç›¸ä¿¡çš„æ–¹å‘ï¼Œå°±æœƒèµ°åˆ°çœŸæ­£çš„è‡ªå·±ã€‚ã€

æ˜€é’å¾®ç¬‘ï¼ŒæŠŠä¾¿åˆ©è²¼æ”¶é€²éŒ¢åŒ…ã€‚å¾æ­¤ï¼Œå¥¹ä¸å†éš¨æ³¢é€æµï¼Œä¹Ÿä¸å†ææ‡¼æ”¹è®Šã€‚å› ç‚ºå¥¹çŸ¥é“â€”â€”

æ¯ä¸€æ¬¡åœä¸‹ä¾†çš„å‹‡æ°£ï¼Œéƒ½å¯èƒ½è®“äººç”Ÿæ”¹å¯«ã€‚
"""  # é‡è¤‡è³‡æ–™æ“´å¢èªæ–™

# ========== ä½¿ç”¨ jieba åˆ†è© ==========
words = list(jieba.cut(text))  # ä½¿ç”¨è©èªè€Œéå­—
word_set = list(set(words))    # å”¯ä¸€è©è¡¨
word2idx = {w: i for i, w in enumerate(word_set)}
idx2word = {i: w for w, i in word2idx.items()}

# ========== è¶…åƒæ•¸ ==========
input_size = len(word_set)
hidden_size = 128
num_layers = 1
seq_length = 5
learning_rate = 0.005

# ========== å»ºç«‹è³‡æ–™é›† ==========
def make_dataset(words, seq_length):
    x_data, y_data = [], []
    for i in range(len(words) - seq_length):
        x_seq = words[i:i+seq_length]
        y_seq = words[i+1:i+seq_length+1]
        x_data.append([word2idx[w] for w in x_seq])
        y_data.append([word2idx[w] for w in y_seq])
    return x_data, y_data

x_data, y_data = make_dataset(words, seq_length)
x = torch.LongTensor(x_data)
y = torch.LongTensor(y_data)

# ========== å®šç¾©æ¨¡å‹ ==========
class WordLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(WordLSTM, self).__init__()
        self.embed = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, input_size)

    def forward(self, x):
        x = self.embed(x)
        out, _ = self.lstm(x)
        out = self.fc(out)
        return out

# ========== æ¨¡å‹è¨“ç·´ ==========
model = WordLSTM(input_size, hidden_size, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

print("é–‹å§‹è¨“ç·´...")
for epoch in range(100):
    output = model(x)
    loss = criterion(output.view(-1, input_size), y.view(-1))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch + 1) % 10 == 0:
        print(f"Epoch [{epoch+1}/100], Loss: {loss.item():.4f}")

# ========== æ¸¬è©¦ç”¢ç”Ÿè©èª ==========
start_words = words[:seq_length]
input_test = torch.LongTensor([[word2idx[w] for w in start_words]])
result = start_words.copy()

model.eval()
with torch.no_grad():
    for _ in range(30):  # ç”Ÿæˆ 30 å€‹è©
        output = model(input_test)
        pred = output[:, -1, :].argmax(dim=1).item()
        next_word = idx2word[pred]
        result.append(next_word)
        input_test = torch.LongTensor([[*input_test[0][1:], pred]])

print("\nğŸ”® ç”Ÿæˆè©èªçµæœï¼š")
print(''.join(result))
